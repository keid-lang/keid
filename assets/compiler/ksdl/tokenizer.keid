namespace keidc::ksdl

import (
    core::collections
    core::error
    core::object
    core::string
)

public enum LexerLiteral {
    Bool {
        val: bool
    }
    SignedInteger {
        val: int64
    }
    UnsignedInteger {
        val: uint64
    }
    String {
        val: String
    }
    Char {
        val: char
    }
    Null
}

implement ToString for LexerLiteral {
    toString(): string {
        return match this {
            Bool { val, } => string.format("Bool(", val.toString(), ")")
            SignedInteger { val, } => string.format("SignedInteger(", val.toString(), ")")
            UnsignedInteger { val, } => string.format("UnsignedInteger(", val.toString(), ")")
            String { val, } => string.format("String(", val, ")")
            Char { val, } => string.format("Char(", val.toString(), ")")
            Null => "Null"
        }
    }
}

public enum LexerTokenKind {
    Identifier {
        name: String
    }
    Keyword {
        name: String
    }
    Operator {
        op: String
    }
    Literal {
        lit: LexerLiteral
    }
    Newline
    EOF

    public identifier(): string {
        match this {
            Identifier { name, } => return name
            _ => throw Error.create("LexerTokenKind is not Identifier")
        }
    }

    public keyword(): string {
        match this {
            Keyword { name, } => return name
            _ => throw Error.create("LexerTokenKind is not Keyword")
        }
    }

    public operator(): string {
        match this {
            Operator { op, } => return op
            _ => throw Error.create("LexerTokenKind is not Operator")
        }
    }
}

implement ToString for LexerTokenKind {
    toString(): string {
        return match this {
            Identifier { name, } => string.format("Identifier(", name, ")")
            Keyword { name, } => string.format("Keyword(", name, ")")
            Operator { op, } => string.format("Operator(", op, ")")
            Literal { lit, } => string.format("Literal(", lit.toString(), ")")
            Newline => "Newline"
            EOF => "EOF"
        }
    }
}

public struct LexerToken {
    loc: TokenLocation
    originalLoc: TokenLocation
    kind: LexerTokenKind

    public static create(loc: TokenLocation, kind: LexerTokenKind): LexerToken => new LexerToken { loc, originalLoc = loc, kind, }

    public get name: string => match this.kind {
        Identifier => "identifier"
        Keyword => "keyword"
        Operator => "operator"
        Literal => "literal"
        Newline => "newline"
        EOF => "EOF"
    }
}

implement ToString for LexerToken {
    toString(): string {
        let sb = StringBuilder.empty()
        sb.append(this.kind.toString())
        sb.append(" @ ")
        sb.append(this.loc.toString())
        if this.originalLoc != this.loc {
            sb.append(" (originally @ ")
            sb.append(this.originalLoc.toString())
            sb.append(")")
        }
        return sb.toString()
    }
}

enum StringTokenizerState {
    Normal
    Escape
}

const KEYWORDS: [string] = new string[
    "as", "class", "const", "constructor","destructor", "else", "end macro",
    "enum", "extends", "extern", "for", "function", "get", "if", "implement",
    "import", "in", "interface", "let", "macro", "match", "namespace", "public",
    "return", "set", "static", "struct", "throw", "type", "unsafe", "unreachable",
    "with", "this", "super", "This",
]
const OPERATORS: [string] = new string[
    "==", "!=", "+=", "-=", "*=", "/=", "%=", "<<=", ">>=", "&=", "|=", "^=",
    "+", "-", "*", "/", "%", "<=", ">=", "=>", "<", ">", "!", "...", "??", "?",
    "&", "|", "^", "=", ",", "@", ":", ".", "(", ")", "[", "]", "{", "}", ";",
]
const KEYWORD_OPERATORS: [string] = new string["and", "or", "not", "as", "ref", "deref"]

public function stringifyTokens(tokens: List<LexerToken>): string {
    let sb = StringBuilder.empty()
    for token in tokens {
        match token.kind {
            Identifier { name, } => {
                sb.append(name)
            }
            Keyword { name, } => {
                sb.append(name)
            }
            Operator { op, } => {
                sb.append(op)
            }
            Literal { lit, } => {
                match lit {
                    Bool { val, } => {
                        sb.append(val.toString())
                    }
                    SignedInteger { val, } => {
                        sb.append(val.toString())
                    }
                    UnsignedInteger { val, } => {
                        sb.append(val.toString())
                    }
                    String { val, } => {
                        sb.append('"')
                        sb.append(val.toString())
                        sb.append('"')
                    }
                    Char { val, } => {
                        sb.append("'")
                        sb.append(val.toString())
                        sb.append("'")
                    }
                    Null => {
                        sb.append("null")
                    }
                }
            }
            Newline => {
                sb.append("\n")
            }
            EOF => {}
        }
        sb.append(" ")
    }
    return sb.toString()
}

public function dumpTokens(tokens: List<LexerToken>): string {
    let sb = StringBuilder.empty()
    for token in tokens {
        sb.append("[")
        sb.append(token.toString())
        sb.append("] ")
        match token.kind {
            Newline => {
                sb.append("\n")
            }
            _ => {}
        }
    }
    return sb.toString()
}

function fail(reader: TokenReader, filePath: string) {
    let docPos = reader.documentPosition
    throw Error.create(string.format("unexpected token '", reader.currentCharacter.toString(), "' at ", filePath, ":", docPos.line.toString(), ":", docPos.col.toString()))
}

public function tokenize(reader: TokenReader, filePath: string): List<LexerToken> {
    let tokens = List.empty<LexerToken>()
    let currentToken = StringBuilder.empty()

    reader.isSkipWhitespace = true
    while !reader.isFinished {
        reader.skipWhitespace()
        if reader.currentCharacter == '\n' || reader.startsWith("\r\n") {
            tokens.push(LexerToken.create(TokenLocation.create(reader.cursor, 1), LexerTokenKind.Newline))
            let adv: usize = match reader.currentCharacter {
                '\n' => 1
                _ => 2
            }
            reader.advance(adv)
            continue
        } else if reader.currentCharacter == '"' || reader.currentCharacter == '\'' {
            let start = reader.cursor
            let delimiter = reader.currentCharacter
            reader.advance(1)
            reader.isSkipWhitespace = false
            let str = tokenizeString(reader, delimiter)
            reader.isSkipWhitespace = true
            if delimiter == '\'' && str.length != 1 {
                throw Error.create("character literal must be exactly one character")
            }
            tokens.push(LexerToken.create(TokenLocation.create(start, reader.cursor - start), new LexerTokenKind.Literal {
                lit = match delimiter {
                    '"' => new LexerLiteral.String {
                        val = str
                    }
                    '\'' => new LexerLiteral.Char {
                        val = str.chars[0]
                    }
                }
            }))
            continue
        } else if reader.currentCharacter.isNumber || reader.currentCharacter == '+' || reader.currentCharacter == '-' {
            let c = reader.currentCharacter
            let cursor = reader.cursor
            reader.advance(1)
            if (c == '+' || c == '-') && !reader.currentCharacter.isNumber {
                // if the + or - sign does not have a number after it, retreat and let the call to tokenizeOperator handle it
                reader.retreat(1)
            } else {
                let sb = StringBuilder.empty()
                // if the first token was a plus or minus sign, append it to the buffer and skip to the next token
                if c == '+' || c == '-' {
                    sb.append(c)
                    c = reader.currentCharacter
                }

                let base: uint8 = 10
                // c is either the first token in the string, or the token directly after the plus or minus sign
                if c == '0' {
                    if reader.currentCharacter == 'b' || reader.currentCharacter == 'B' {
                        base = 2
                        reader.advance(1)
                    } else if reader.currentCharacter == 'o' || reader.currentCharacter == 'O' {
                        base = 8
                        reader.advance(1)
                    } else if reader.currentCharacter == 'x' || reader.currentCharacter == 'X' {
                        base = 16
                        reader.advance(1)
                    }
                }

                // if the first token was not used to define the base of the number, append it to the buffer
                if base == 10 {
                    sb.append(c)
                }

                reader.isSkipWhitespace = false
                while isCharWithinBase(reader.currentCharacter, base) {
                    c = reader.currentCharacter
                    reader.advance(1)
                    sb.append(c)
                }
                if isCharIdentifierComponent(reader.currentCharacter) {
                    fail(reader, filePath)
                }
                reader.isSkipWhitespace = true

                let s = sb.toString()
                let lit = match s.startsWith("-") {
                    true => new LexerLiteral.SignedInteger {
                        val = Int64.parse(s, base)
                    }
                    false => new LexerLiteral.UnsignedInteger {
                        val = UInt64.parse(s, base)
                    }
                }

                tokens.push(LexerToken.create(TokenLocation.create(cursor, reader.cursor - cursor), new LexerTokenKind.Literal {
                    lit
                }))
                continue
            }
        }
        if advanceComment(reader) {
            continue
        }
        let pos = reader.cursor
        if advanceKeyword(reader, "null") {
            tokens.push(LexerToken.create(TokenLocation.create(pos, 4), new LexerTokenKind.Literal {
                lit = LexerLiteral.Null
            }))
            continue
        }
        if advanceKeyword(reader, "true") {
            tokens.push(LexerToken.create(TokenLocation.create(pos, 4), new LexerTokenKind.Literal {
                lit = new LexerLiteral.Bool {
                    val = true
                }
            }))
            continue
        }
        if advanceKeyword(reader, "false") {
            tokens.push(LexerToken.create(TokenLocation.create(pos, 5), new LexerTokenKind.Literal {
                lit = new LexerLiteral.Bool {
                    val = false
                }
            }))
            continue
        }
        if tokenizeOperator(reader, tokens) {
            continue
        }
        if tokenizeKeywordOperator(reader, tokens) {
            continue
        }
        if tokenizeKeyword(reader, tokens) {
            continue
        }
        if tokenizeIdentifier(reader, tokens) {
            continue
        }

        fail(reader, filePath)
    }

    return tokens
}

function advanceComment(reader: TokenReader): bool {
    if reader.startsWith("//") {
        reader.advance(2)
        while reader.currentCharacter != '\n' {
            reader.advance(1)
        }
        return true
    }
    return false
}

function advanceKeyword(reader: TokenReader, keyword: string): bool {
    if reader.startsWith(keyword) {
        reader.advance(keyword.length)
        if !isCharIdentifierComponent(reader.currentCharacter) {
            return true
        } else {
            reader.retreat(keyword.length)
        }
    }
    return false
}

function tokenizeKeyword(reader: TokenReader, tokens: List<LexerToken>): bool {
    for keyword in KEYWORDS {
        let cursor = reader.cursor
        if advanceKeyword(reader, keyword) {
            tokens.push(LexerToken.create(TokenLocation.create(cursor, keyword.length), new LexerTokenKind.Keyword {
                name = keyword
            }))
            return true
        }
    }
    return false
}

function tokenizeIdentifier(reader: TokenReader, tokens: List<LexerToken>): bool {
    if isCharIdentifierComponent(reader.currentCharacter) {
        let cursor = reader.cursor
        let sb = StringBuilder.withCapacity(1)
        while !reader.isFinished && isCharIdentifierComponent(reader.currentCharacter) {
            sb.append(reader.currentCharacter)
            reader.advance(1)
        }
        let ident = sb.toString()
        tokens.push(LexerToken.create(TokenLocation.create(cursor, ident.length), new LexerTokenKind.Identifier {
            name = ident
        }))
        return true
    }
    return false
}

function tokenizeOperator(reader: TokenReader, tokens: List<LexerToken>): bool {
    for operator in OPERATORS {
        if reader.startsWith(operator) {
            tokens.push(LexerToken.create(TokenLocation.create(reader.cursor, operator.length), new LexerTokenKind.Operator {
                op = operator
            }))
            reader.advance(operator.length)
            return true
        }
    }
    return false
}

function tokenizeKeywordOperator(reader: TokenReader, tokens: List<LexerToken>): bool {
    for op in KEYWORD_OPERATORS {
        let cursor = reader.cursor
        if advanceKeyword(reader, op) {
            tokens.push(LexerToken.create(TokenLocation.create(cursor, op.length), new LexerTokenKind.Operator {
                op
            }))
            return true
        }
    }
    return false
}

function tokenizeString(reader: TokenReader, delimiter: char): string {
    let state = StringTokenizerState.Normal
    let builder = StringBuilder.empty()
    while !reader.isFinished {
        let c = reader.currentCharacter
        reader.advance(1)

        match state {
            Normal => {
                if c == delimiter {
                    return builder.toString()
                } else if c == '\\' {
                    state = StringTokenizerState.Escape
                } else {
                    builder.append(c)
                }
            }
            Escape => {
                let m = match c {
                    '"' => '"'
                    '\'' => '\''
                    '\\' => '\\'
                    '0' => '\0'
                    'n' => '\n'
                    'r' => '\r'
                    't' => '\t'
                    _ => throw Error.create("Invalid escape sequence")
                }
                builder.append(m)
                state = StringTokenizerState.Normal
            }
        }
    }
    throw Error.create("unexpected EOF while tokenizing string")
}

function isCharIdentifierComponent(c: char): bool {
    return (c >= 'A' && c <= 'Z') || (c >= 'a' && c <= 'z') || (c >= '0' && c <= '9') || c == '_' || c == '$' || c == '#'
}

function isCharWithinBase(c: char, base: uint8): bool {
    let numberCheck = c >= '0' && c < (base as char + '0')
    if numberCheck {
        return true
    }
    // all numbers in base [2, 10] should pass `numberCheck`
    if base <= 10 {
        false
    }
    return (c >= 'a' && c <= (base as char + 'a')) || (c >= 'A' && c <= (base as char + 'A'))
}
